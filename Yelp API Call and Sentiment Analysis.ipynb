{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Fusion API Call and Naive Bayes Classifier Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared by Denisa Bani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter file will explore how to make a Yelp Fusion API call in order to download and analyze reviews and ratings for restaurants in the Toronto area. A naive bayes classifier will be used to predict whether the rating from a review will be positive or negative, where a positive rating is defined as a rating greater than 3 and a negative rating is 3 or lower. We will then find that using Yelp Fusion has many limitations and then explore how to web scrape using BeautifulSoup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the Yelp Fusion page: https://www.yelp.com/developers/v3/manage_app and create an app in order to obtain a Yelp token. My Yelp token was obtained and saved as the variable: ` YELP_TOKEN ` but this line of code is hidden for security purposes.\n",
    "\n",
    "After the token is obtained, we can make a request for restaurants in  pages 1-50 within the Toronto area and then verify whether the request was valid (aka, make sure there were no issues with the token verification). We want to see the status code '200' to verify that this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://api.yelp.com/v3/businesses/search?location=Toronto&limit=50\", headers={\"Authorization\": \"Bearer %s\" % YELP_TOKEN})\n",
    "print(r.status_code, r.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifier using the Yelp Fusion API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After verifiying that the verification token is functional, we need to scrape *just* the reviews and ratings as that is all we need for our sentiment analysis.There are a couple limitations to using Yelp's API, as documented on this [page](https://www.yelp.ca/developers/faq). The limitations that will specifically impact the classifier is the restriction to 1000 businesses and only being able to retrieve review excerpts that are 160 characters long. This will limit the performance of the naive bayes classifier as a lot of information is potentially being lost.  \n",
    "\n",
    "The reviews will be compliled into a list of tuples where a tuple contains the review as the first entry and the rating as the second entry. The list will be the variable `review_labels`. Afterwards, the review in the tuple will be broken down into a list and the rating will be converted into a binary \"postive\" or \"negative\" rating - this will be stored as `review_features`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_labels = []\n",
    "pages = np.arange(51,1001,50)\n",
    "for page in pages:\n",
    "    r = requests.get(\"https://api.yelp.com/v3/businesses/search?location=Toronto&offset=\"+str(page), headers={\"Authorization\": \"Bearer %s\" % YELP_TOKEN})\n",
    "    for business in r.json()['businesses']:\n",
    "        reviews = requests.get(\"https://api.yelp.com/v3/businesses/%s/reviews\" % business['id'], headers={\"Authorization\": \"Bearer %s\" % YELP_TOKEN}).json()\n",
    "        for review in reviews['reviews']:\n",
    "            rev = review['text'].rstrip('.')\n",
    "            review_labels.append((rev.replace('\\n\\n',''), review['rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_features = [(x.split(' '), 'positive' if y > 3 else 'negative') for (x, y) in review_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 760, Testing: 380\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "import nltk.sentiment.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "random.shuffle(review_features)\n",
    "training_docs = review_features[:int(len(review_features)*2/3)]\n",
    "test_docs = review_features[int(len(review_features)*2/3):]\n",
    "\n",
    "print(\"Training: %d, Testing: %d\" % (len(training_docs), len(test_docs)))\n",
    "\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "\n",
    "all_words_neg = sentim_analyzer.all_words([nltk.sentiment.util.mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.7210526315789474\n",
      "F-measure [negative]: 0.32911392405063294\n",
      "F-measure [positive]: 0.8239202657807309\n",
      "Precision [negative]: 0.40625\n",
      "Precision [positive]: 0.7848101265822784\n",
      "Recall [negative]: 0.2765957446808511\n",
      "Recall [positive]: 0.8671328671328671\n"
     ]
    }
   ],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "sentim_analyzer.add_feat_extractor(nltk.sentiment.util.extract_unigram_feats, unigrams=unigram_feats)\n",
    "\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(test_docs)\n",
    "\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "     print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72% accuracy isn't the best. Let's see if we can do better by trying other methods that won't limit our web scraping like Yelp Fusion does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifier using BeautifulSoup for Review Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of being limited by the number of characters in a review from Yelp's API, let's try using BeautifulSoup and see the impact on our classification results. A really great and relevant blog post on the subject can be found [here](https://www.octoparse.com/blog/web-scraping-using-python) and [here](https://www.youtube.com/watch?v=r3-v81c2Oew).\n",
    "\n",
    "Note that Yelp uses Javascipt to display its webpages, which means that simply inspecting the page and using BeautifulSoup's `find_all` won't work since BeautifulSoup doesn't run Javascripts. Some work-arounds that were researched was to use Selenium with BeautifulSoup, but this fancy technique isn't necessary as the information we need can be obtained without Javascripts. Here is a [link](https://towardsdatascience.com/web-scraping-using-selenium-and-beautifulsoup-99195cd70a58) to explore the Selenium option if you want.\n",
    "\n",
    "Instead, an easier alternative is to forego using \"Inspect\" on your web browser, and use \"View Page Source\". Note that I'm using Firefox so this may have another name on other browsers. Now we can refer to the correct tags that BeautifulSoup would also recognize. A loop will be used to click through and parse through the different pages to collect as many reviews and ratings as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page = 0\n",
    "reviews = []\n",
    "ratings = []\n",
    "combo =[]\n",
    "\n",
    "while page <= 280:\n",
    "    \n",
    "    url = 'https://www.yelp.com/biz/frescos-fish-and-chips-toronto?start=' + str(page)\n",
    "    \n",
    "    ourURL = urllib.request.urlopen(url)\n",
    "    \n",
    "    soup = BeautifulSoup(ourURL, 'html.parser')\n",
    "    \n",
    "    #print(soup.prettify())\n",
    "    \n",
    "    reviews_html = soup.find_all(\"p\", {\"itemprop\":\"description\"})\n",
    "    ratings_html = soup.find_all(\"div\", {\"itemprop\":\"reviewRating\"})\n",
    "    \n",
    "    #Time to clean up \n",
    "    \n",
    "    for review in reviews_html:\n",
    "        rev= str(review).replace('\\n\\n','')\n",
    "        reviews.append(rev[26:-12])\n",
    "        \n",
    "    \n",
    "    for rating in ratings_html:\n",
    "        ratings.append(float(rating.meta.get('content')))\n",
    "       \n",
    "    \n",
    "    for i in range(len(reviews)):\n",
    "        combo.append((reviews[i],ratings[i]))\n",
    "        \n",
    "    page += 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_features= [(x.split(' '), 'positive' if y > 3 else 'negative') for (x, y) in combo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1600, Testing: 800\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(review_features)\n",
    "training_docs = review_features[:int(len(review_features)*2/3)]\n",
    "test_docs = review_features[int(len(review_features)*2/3):]\n",
    "\n",
    "print(\"Training: %d, Testing: %d\" % (len(training_docs), len(test_docs)))\n",
    "\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "\n",
    "all_words_neg = sentim_analyzer.all_words([nltk.sentiment.util.mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.9875\n",
      "F-measure [negative]: 0.9514563106796117\n",
      "F-measure [positive]: 0.9928263988522238\n",
      "Precision [negative]: 1.0\n",
      "Precision [positive]: 0.9857549857549858\n",
      "Recall [negative]: 0.9074074074074074\n",
      "Recall [positive]: 1.0\n"
     ]
    }
   ],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "sentim_analyzer.add_feat_extractor(nltk.sentiment.util.extract_unigram_feats, unigrams=unigram_feats)\n",
    "\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(test_docs)\n",
    "\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "     print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a major improvement over using the Yelp Fusion API as we are no longer limited by the length of the reviews that we can retrieve nor by the number of reviews we can obtain. Our accuracy is now a solid 99%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
